%% LyX 2.0.0 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\makeatother

\makeatother

\usepackage{babel}
\begin{document}

\title{Performance Report on EC2 Instances for Memory Operations}


\author{Vineet Kumar, Phuc Xuan Nguyen}

\maketitle

\section{RAM Access Time}


\section{RAM Bandwidth}
\begin{description}
\item [{a)}] Prediction - To Predict RAM Bandwidth we need to multiply
the memory bus size with the memory clock speed. Our bus size is 64
bits which is 8 bytes. And our memory clock speed is 800 MhZ - which
gives us a maximum theoretical transfer rate of 6.4 GB /s. We predict
that the write bandwidth would be less than this as it needs to handle
issue of page cache eviction
\item [{b)}] Experiment for Read Bandwidth : \end{description}
\begin{itemize}
\item We create a large array in the memory
\item In a tight unrolled for loop - we access array elements at interval
of L2 cache size and add them up. Since our loop is unrolled 4 times,
this operation should cause 8 memory read operations from DRAM to
L2
\item We measure beforehand amount of data transfered between DRAM and L2
for accessing 4 bytes. We found this to be 4K.
\item We stop the timer that was started before 8 memory read opertaions.
We would have read 32K in one loop iteration. We divide this by time
taken and get the memory bandwidth\end{itemize}
\begin{description}
\item [{c)}] Experiment for Write Bandwidth :\end{description}
\begin{itemize}
\item We create a large array in the memory
\item In a tight unrolled for loop - We add 2 fixed integers and store the
result at an interval of L2 size in our array
\item We stop the timer that was started before 8 memory write operations\end{itemize}
\begin{description}
\item [{d)}] Block Transfer:\end{description}
\begin{itemize}
\item We do a L2 size block transfer from one memory location to another
in a loop. We measure time it takes to do the transfer
\item This transfer will involve one read and possibly 2 write operations.
Thus we expect the bandwidth seen by this operation as average of
read and write bandwidth \end{itemize}
\begin{description}
\item [{e)}] Results:
\item [{%
\begin{tabular}{|c|c|c|c|}
\hline 
 & CPU Cycles & Std. Deviation & Bandwidth GB /s\tabularnewline
\hline 
\hline 
Read & 41,518 & 0.05 \% & 2.076\tabularnewline
\hline 
Write & 98, 781 & 0.03\% & 0.878\tabularnewline
\hline 
Block Transfer & 49,223,872 & 0.023\% & 1.279\tabularnewline
\hline 
\end{tabular}}]~
\item [{f)}] Analysis:\end{description}
\begin{itemize}
\item Block tarnsfer test results macth up with what we expected. We expected
the bandwidth to be average of read and write bandwidth numbers
\item However, out orginal estimation seems to be off by quite a lot. We
expected to see a bandwidth of 6GB /s and saw 2 GB / s - We can attribute
it to the fact that we are running it on top of hypervisor and thus
page fault time could take more time as PTEs need to be installed
in the hypervisor's shadow page tables.
\end{itemize}

\section{Page Fault Service Time :}
\begin{description}
\item [{a)}] Prediction :- To estimate page fault service time we wanted
to measure raw time of reading a page from disk. This is hard to do
from user space - but we can measure tiem to read a file without using
page cache. This would still include an overhead of copying data between
buffers - so while making an estimate we should note that this estimate
at best would be upper bound on page fault service time. \end{description}
\begin{itemize}
\item We measure the time to do a disk read by measuring cost of doing a
direct read using 'dd' and redirecting output to /dev/null
\item We notice that this gives us a speed of 5.1 MB/s -> Note that this
is a gross underestimate - it includes time to copy data between buffers
\item Based on the clock speed and bandwidth and page size - we estimate
that around 2.1 million cycles will be needed\end{itemize}
\begin{description}
\item [{b)}] Experiment :- Our aim is to measure the page fault service
time. This means we want to measure the time it takes to do a major
page fault. This includes the time needed to bring the page from the
disk. We use an mmap (followed by no read aheads) to measure the same.
This works as mmap does a on demand page fault. Page is faulted in
only on first access. Below is a detailed description of our experiment
:\end{description}
\begin{itemize}
\item Create a Large File ( > 1.5 G) using a simple dd tool
\item Determine page size by calling 'getconf PAGESIZE' - This gives us
page size of 4096 bytes or 4K on our system
\item Call a posix\_fadvise with flag FADVISE\_DONTNEED. We do this to make
sure that a page is always brought in from the disk and not cached.
This is equivalent of dropping the page cache (by calling echo 1 >
/proc/sys/vm/drop\_cache) 
\item Memory Map the file using 'mmap' system call with flags 'MAP\_PRIVATE'.
We do this as we just want to do a read on the file.
\item Do a madvise on the memory mapped area using MADV\_RANDOM. Do tests
both with and without MADV\_RANDOM
\item Start timer
\item Access a random page by reading a page address. This will cause a
major page fault.
\item End timer. Report the time between start and end of timer.\end{itemize}
\begin{description}
\item [{c)}] Results : The table below shows results for reading one page
(4096) . Column 1 and 2 show results without madvise and Columns 3
and 4 show results with madvise
\item [{%
\begin{tabular}{|c|c|c|c|}
\hline 
 & Avg. CPU Cycles & Std. Deviation(CPU Cycles) & Time\tabularnewline
\hline 
\hline 
Page Fault (w/o) madvise & 58,578 & 0.7625\% & 21ms\tabularnewline
\hline 
Page Fault (with) madvise & 197,190 & 0.1898\% & 73 ms\tabularnewline
\hline 
\end{tabular}}]~
\item [{d)}] Analysis :-\end{description}
\begin{itemize}
\item Our estimate was 2.1 Million CPU Cycles and what we got was 0.2 Million
cycles. As explained above, our estimate had some overhead due to
copying of data between buffers. Thus we expected the actual time
to be smaller
\item Its interesting to note that if we dont give an madvise 'advice' for
our memory map region, we get better numbers. But those numbers are
most likely incorrect. We think if we dont do madvise, we hit pages
in page cache and thus the fault time does not remain as major page
fault service rate.
\item To conclude, to measure page fault service rate, we need to ensure
that the page is brough in by reading from disk\end{itemize}

\end{document}
